package recache

import (
	"bytes"
	"compress/gzip"
	"crypto/sha1"
	"io"
	"sync"
)

var (
	buffPool = sync.Pool{
		New: func() interface{} {
			return make([]byte, 4<<10)
		},
	}
)

// Provides utility methods for building record buffers and recursive record
// trees
type RecordWriter struct {
	compressing     bool // Currently compressing data into a buffer
	cache, frontend uint
	key             Key
	gzWriter        *gzip.Writer
	pending         bytes.Buffer
	data            []component
}

// Write non-gzipped data to the record for storage
func (rw *RecordWriter) Write(p []byte) (n int, err error) {
	if !rw.compressing {
		// Initialize or reset pipeline state.
		// Reuse allocated resources, if possible
		rw.pending.Reset()
		if rw.gzWriter == nil {
			rw.gzWriter = gzip.NewWriter(&rw.pending)
		} else {
			rw.gzWriter.Reset(&rw.pending)
		}
		rw.compressing = true
	}
	return rw.gzWriter.Write(p)
}

// Read non-gzipped data from r and write it to the record for storage
func (rw *RecordWriter) ReadFrom(r io.Reader) (n int64, err error) {
	var (
		m   int
		buf = buffPool.Get().([]byte)
	)
	defer buffPool.Put(buf)

	for {
		m, err = r.Read(buf)
		n += int64(m)
		switch err {
		case nil:
			_, err = rw.Write(buf[:m])
			if err != nil {
				return
			}
		case io.EOF:
			err = nil
			return
		default:
			return
		}
	}
}

// Include data from passed frontend by key and bind it to rw.
// The record generated by rw will automatically be evicted from its parent
// cache on eviction of the included record.
func (rw *RecordWriter) Include(f *Frontend, k Key) (err error) {
	// Finish any previous buffer writes
	err = rw.flushPending(false)
	if err != nil {
		return
	}

	rec, err := rw.bind(f, k)
	if err != nil {
		return
	}

	rw.data = append(rw.data, recordReference{
		componentCommon: componentCommon{
			hash: rec.hash,
		},
		record: rec,
	})

	return
}

func (rw *RecordWriter) bind(f *Frontend, k Key) (rec *record, err error) {
	rec, err = f.getGeneratedRecord(k)
	if err != nil {
		return
	}

	registerInclusion(
		intercacheRecordLocation{
			cache: rw.cache,
			recordLocation: recordLocation{
				frontend: rw.frontend,
				key:      rw.key,
			},
		},
		intercacheRecordLocation{
			cache: f.cache.id,
			recordLocation: recordLocation{
				frontend: f.id,
				key:      k,
			},
		},
	)

	return
}

// Bind to record from passed frontend by key and return a consumable stream
// of the retrieved record.
// The record generated by rw will automatically be evicted from its parent
// cache on eviction of the included record.
func (rw *RecordWriter) Bind(f *Frontend, k Key) (Streamer, error) {
	rec, err := rw.bind(f, k)
	if err != nil {
		return nil, err
	}
	return recordDecoder{rec}, nil
}

// Bind to record from passed frontend by key and decode it as JSON into dst.
// The record generated by rw will automatically be evicted from its parent
// cache on eviction of the included record.
func (rw *RecordWriter) BindJSON(
	f *Frontend,
	k Key,
	dst interface{},
) (err error) {
	s, err := rw.Bind(f, k)
	if err != nil {
		return
	}
	return s.DecodeJSON(dst)
}

// Flush any pending buffer.
// final: this is the final flush and copying of buffer is not required
func (rw *RecordWriter) flushPending(final bool) (err error) {
	if rw.compressing {
		err = rw.gzWriter.Close()
		if err != nil {
			return
		}

		var buf buffer
		if final {
			buf.data = rw.pending.Bytes()
		} else {
			buf.data = make([]byte, rw.pending.Len())
			copy(buf.data, rw.pending.Bytes())
		}
		buf.hash = sha1.Sum(buf.data)

		rw.data = append(rw.data, buf)
		rw.compressing = false
	}
	return
}
